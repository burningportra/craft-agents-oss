{
  "created_at": "2026-02-08T03:19:45.404931Z",
  "epic": {
    "data": {
      "branch_name": "fn-4-real-ai-epic-chat-with-project-aware",
      "completion_review_status": "unknown",
      "completion_reviewed_at": null,
      "created_at": "2026-02-08T03:17:00.962559Z",
      "default_impl": null,
      "default_review": null,
      "default_sync": null,
      "depends_on_epics": [],
      "id": "fn-4-real-ai-epic-chat-with-project-aware",
      "next_task": 1,
      "plan_review_status": "unknown",
      "plan_reviewed_at": null,
      "spec_path": ".flow/specs/fn-4-real-ai-epic-chat-with-project-aware.md",
      "status": "open",
      "title": "Real AI Epic Chat with Project-Aware Context",
      "updated_at": "2026-02-08T03:17:37.879122Z"
    },
    "spec": "# Real AI Epic Chat with Project-Aware Context\n\n## Overview\n\nThe epic chat panel currently has `/plan` wired to a real planning agent (Anthropic SDK), but `/interview`, `/review`, and free-form chat all use `simulateAIResponse()` which returns hardcoded template strings. The empty state is minimal (icon + two lines of text). There is no project awareness or cross-project knowledge.\n\nThis epic replaces the mock responses with real streaming LLM calls, adds a smart project-aware empty state with dynamic starter prompts, and injects cross-project learnings into the chat context.\n\n## Scope\n\n**In scope:**\n- New `epic-chat-agent.ts` module in main process with streaming Anthropic SDK calls\n- New IPC channel(s) for epic chat streaming (send, status events, abort)\n- Replace `simulateAIResponse()` with real IPC\u2192agent calls for all command types\n- Streaming rendering via existing `updateLastMessage` hook\n- Stop/cancel button with AbortController\n- Conversation history sent to LLM for multi-turn context\n- Smart empty state with dynamic starters based on epic state\n- Project context injection (learnings.md, .flow/memory/, project metadata)\n- Cross-project knowledge from registered flow projects\n- Error handling UI (rate limit, network, auth)\n- Epic-switch mid-stream abort + cleanup\n\n**Out of scope:**\n- Tool use capability for chat agent (future enhancement)\n- AISuggestionSidebar LLM upgrade (separate epic)\n- Image/file attachments in chat\n- Token usage display in chat UI\n- `/plan` streaming upgrade (already works, different architecture)\n\n## Approach\n\nFollow the existing `planning-agent.ts` pattern for the new agent but add streaming. Use the app's credential manager (not raw env var) and user's configured model (not hardcoded). Single IPC channel with type discriminator for all chat commands. Renderer owns conversation history (IndexedDB), sends recent N messages over IPC per request.\n\n### Architecture\n\n```\nRenderer (EpicChatPanel)              Main Process\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    IPC     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 handleSend()         \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 FLOW_EPIC_CHAT_SEND  \u2502\n\u2502                      \u2502            \u2502  \u251c\u2500 epic-chat-agent   \u2502\n\u2502 onChatStatus()       \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  \u2502  \u251c\u2500 build context  \u2502\n\u2502  \u251c\u2500 text_delta       \u2502  stream    \u2502  \u2502  \u251c\u2500 system prompt  \u2502\n\u2502  \u251c\u2500 text_complete    \u2502  events    \u2502  \u2502  \u2514\u2500 stream call    \u2502\n\u2502  \u251c\u2500 error            \u2502            \u2502  \u2514\u2500 webContents.send  \u2502\n\u2502  \u2514\u2500 abort            \u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                      \u2502    IPC     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 abortChat()          \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 FLOW_EPIC_CHAT_ABORT \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Key decisions\n- **Single channel with type field** (not per-command channels) \u2014 simpler, all commands share streaming infrastructure\n- **Renderer owns history** \u2014 IndexedDB in renderer, sends last N messages over IPC. Main process is stateless per request\n- **Streaming from V1** \u2014 `updateLastMessage` hook is already wired, streaming is essential for chat UX\n- **Use credential manager** \u2014 not `ANTHROPIC_API_KEY` env var, consistent with main session auth\n- **User's configured model** \u2014 from `models.ts` config, not hardcoded\n- **No tool use V1** \u2014 text-in/text-out, tools can be added later\n- **Rule-based starters + LLM context** \u2014 empty state starters are rule-based (instant), but the agent's system prompt includes rich project context\n\n## Quick commands\n\n```bash\n# Dev server\ncd apps/electron && bun run dev\n\n# Type check\ncd apps/electron && bun run typecheck\n\n# Verify IPC channels compile\ncd apps/electron && bunx tsc --noEmit src/shared/types.ts\n```\n\n## Acceptance\n\n- [ ] `/interview`, `/review`, and free-form chat return real LLM responses (not hardcoded)\n- [ ] Responses stream token-by-token in the chat UI\n- [ ] Stop button cancels in-flight LLM requests\n- [ ] Switching epics aborts in-flight stream and cleans up\n- [ ] Conversation history (last N messages) is sent to LLM for multi-turn context\n- [ ] Empty chat shows project-aware starter prompts based on epic state\n- [ ] Clicking a starter prompt sends the message\n- [ ] System prompt includes epic spec, task state, project metadata, and learnings\n- [ ] Cross-project learnings from registered projects are included in context\n- [ ] Error states (rate limit, network, auth) show actionable UI\n- [ ] No API keys exposed in renderer process\n- [ ] TypeScript compiles with no errors\n\n## References\n\n- `apps/electron/src/main/lib/planning-agent.ts` \u2014 existing real agent pattern\n- `apps/electron/src/renderer/components/tasks/EpicChatPanel.tsx:620-674` \u2014 `simulateAIResponse()` to replace\n- `apps/electron/src/renderer/components/tasks/EpicChatHistory.tsx:266-279` \u2014 `updateLastMessage` for streaming\n- `apps/electron/src/shared/types.ts:782-784` \u2014 existing IPC channel pattern\n- `apps/electron/src/main/ipc.ts:2686-2726` \u2014 existing plan IPC handler\n- `apps/electron/src/preload/index.ts:509-521` \u2014 existing preload bindings\n- `packages/shared/src/agent/learnings.ts` \u2014 workspace learnings system\n- `packages/shared/src/credentials/backends/secure-storage.ts` \u2014 credential manager\n- `packages/shared/src/config/models.ts` \u2014 model configuration\n"
  },
  "epic_id": "fn-4-real-ai-epic-chat-with-project-aware",
  "schema_version": 2,
  "tasks": [
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-08T03:17:42.915285Z",
        "depends_on": [],
        "epic": "fn-4-real-ai-epic-chat-with-project-aware",
        "id": "fn-4-real-ai-epic-chat-with-project-aware.1",
        "priority": null,
        "spec_path": ".flow/tasks/fn-4-real-ai-epic-chat-with-project-aware.1.md",
        "status": "todo",
        "title": "Epic Chat Agent + IPC Streaming Layer",
        "updated_at": "2026-02-08T03:18:15.914236Z"
      },
      "id": "fn-4-real-ai-epic-chat-with-project-aware.1",
      "runtime": null,
      "spec": "# fn-4-real-ai-epic-chat-with-project-aware.1 Epic Chat Agent + IPC Streaming Layer\n\n## Description\nCreate a new `epic-chat-agent.ts` module in the main process that handles all epic chat LLM calls (free-form, `/interview`, `/review`) with streaming responses. Add the corresponding IPC channels following the established 3-file pattern.\n\n**Size:** M\n**Files:**\n- `apps/electron/src/main/lib/epic-chat-agent.ts` (NEW)\n- `apps/electron/src/shared/types.ts` (add IPC channels + ElectronAPI methods)\n- `apps/electron/src/main/ipc.ts` (add handler)\n- `apps/electron/src/preload/index.ts` (add bridge methods)\n\n## Approach\n\n- Follow `planning-agent.ts` pattern at `apps/electron/src/main/lib/planning-agent.ts:242-297` for Anthropic SDK usage\n- Add two IPC channels: `FLOW_EPIC_CHAT_SEND` (renderer\u2192main, invoke) and `FLOW_EPIC_CHAT_STATUS` (main\u2192renderer, streaming events via `webContents.send`)\n- Add `FLOW_EPIC_CHAT_ABORT` channel for cancellation\n- Agent function signature: `executeChat(params: { epicId, commandType, message, history, workspaceRoot, window })` where `commandType` is `'interview' | 'review' | 'chat'`\n- Build system prompt per command type using `getCommandSystemPrompt()` pattern from `ChatActionButtons.tsx:122-133` but richer \u2014 include epic spec (via `readEpicSpec`), task list with statuses, project metadata (package.json name, README first paragraph via `FLOW_READ_PROJECT_CONTEXT` handler at `ipc.ts:2882-2919`)\n- Use `client.messages.stream()` (not `.create()`) for streaming \u2014 forward `text` events as `{ type: 'text_delta', text }` and `end` as `{ type: 'text_complete' }` via `webContents.send('flow:epic-chat-status', event)`\n- Track active streams in `Map<string, AbortController>` keyed by epicId \u2014 abort previous stream if new request arrives for same epic\n- Use app's credential path: get API key from credential manager (follow pattern in `sessions.ts` for Anthropic client creation), NOT raw `new Anthropic()` with env var\n- Use user's configured model from `packages/shared/src/config/models.ts` \u2014 `getDefaultModel()` or similar\n- Accept `history: Array<{ role, content }>` parameter \u2014 last 20 messages max, renderer truncates before sending\n- Read `learnings.md` from workspace root if it exists, include in system prompt under \"Project Learnings\" section\n\n## Key context\n\n- Existing IPC channel pattern: `shared/types.ts:766-799` defines channels, `ipc.ts:2686-2726` handles them, `preload/index.ts:509-521` bridges them\n- `updateLastMessage` in `EpicChatHistory.tsx:266-279` is already wired for streaming but unused \u2014 Task 2 will connect it\n- Planning agent hardcodes `claude-sonnet-4-20250514` at `planning-agent.ts:245` \u2014 do NOT follow this pattern, use model config\n- ElectronAPI interface at `shared/types.ts:881` \u2014 add `flowEpicChatSend`, `onFlowEpicChatStatus`, `flowEpicChatAbort`\n## Acceptance\n- [ ] `epic-chat-agent.ts` exists with `executeChat()` function accepting epicId, commandType, message, history, workspaceRoot\n- [ ] Three IPC channels added: `FLOW_EPIC_CHAT_SEND`, `FLOW_EPIC_CHAT_STATUS`, `FLOW_EPIC_CHAT_ABORT`\n- [ ] Preload exposes `flowEpicChatSend()`, `onFlowEpicChatStatus()`, `flowEpicChatAbort()`\n- [ ] Agent streams text deltas via `webContents.send` (not request-response)\n- [ ] AbortController tracked per epicId; new request aborts previous stream\n- [ ] System prompt varies by commandType (interview=requirements elicitation, review=epic analysis, chat=general assistance)\n- [ ] System prompt includes epic spec, task list with statuses, project name from package.json\n- [ ] Uses credential manager for API key (not env var)\n- [ ] Uses user's configured model (not hardcoded)\n- [ ] Accepts conversation history (up to 20 messages) for multi-turn context\n- [ ] Error events sent for rate limit (429), auth (401), network failures with actionable error types\n- [ ] TypeScript compiles with no errors (`bun run typecheck`)\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-08T03:17:48.153403Z",
        "depends_on": [
          "fn-4-real-ai-epic-chat-with-project-aware.1"
        ],
        "epic": "fn-4-real-ai-epic-chat-with-project-aware",
        "id": "fn-4-real-ai-epic-chat-with-project-aware.2",
        "priority": null,
        "spec_path": ".flow/tasks/fn-4-real-ai-epic-chat-with-project-aware.2.md",
        "status": "todo",
        "title": "Wire Chat UI to Streaming Agent",
        "updated_at": "2026-02-08T03:18:37.931750Z"
      },
      "id": "fn-4-real-ai-epic-chat-with-project-aware.2",
      "runtime": null,
      "spec": "# fn-4-real-ai-epic-chat-with-project-aware.2 Wire Chat UI to Streaming Agent\n\n## Description\nReplace `simulateAIResponse()` in `EpicChatPanel.tsx` with real IPC calls to the new epic chat agent. Implement streaming rendering, stop button, conversation history management, epic-switch abort, and error handling UI.\n\n**Size:** M\n**Files:**\n- `apps/electron/src/renderer/components/tasks/EpicChatPanel.tsx` (major changes)\n- `apps/electron/src/renderer/components/tasks/EpicChatHistory.tsx` (wire `updateLastMessage`)\n- `apps/electron/src/renderer/components/tasks/ChatActionButtons.tsx` (minor: remove client-side system prompts)\n\n## Approach\n\n- Delete `simulateAIResponse()` function at `EpicChatPanel.tsx:620-674`\n- In `handleSend()` at `EpicChatPanel.tsx:250-310`:\n  - For ALL commands (interview, review, chat): call `window.electronAPI.flowEpicChatSend({ epicId, commandType, message, history })` where `history` is the last 20 messages from `messages` state\n  - Remove the branching at L281-283 that only routes `/plan` to real IPC\n  - Keep `/plan` routing to existing `FLOW_EPIC_PLAN` channel (don't change that)\n- Set up streaming listener: `window.electronAPI.onFlowEpicChatStatus((event) => { ... })` in `useEffect`, following the pattern at `EpicChatPanel.tsx:166-190` for plan status events\n  - On `text_delta`: call `updateLastMessage(prevContent + event.text)` from `useEpicChatHistory`\n  - On `text_complete`: call `saveMessages()`, set `isProcessing(false)`\n  - On `error`: show error in chat as a special error message bubble with retry button\n- Add a \"Stop\" button visible during `isProcessing` \u2014 calls `window.electronAPI.flowEpicChatAbort(epicId)`\n- Handle epic switching: in `useEffect` cleanup or when `epicId` changes, call abort if `isProcessing`\n- Auto-scroll: only scroll to bottom when user is near bottom (within 100px), show \"scroll to bottom\" indicator when new messages arrive and user is scrolled up\n- Before sending history over IPC, truncate to last 20 messages to stay within token budget\n\n## Key context\n\n- `updateLastMessage` at `EpicChatHistory.tsx:266-279` is already wired but never called \u2014 this is the streaming hook\n- `saveMessages` has a stale closure issue \u2014 it captures `messages` state. Call it AFTER the final `setMessages` that includes the completed assistant message, or use the `messages` ref pattern\n- `isProcessing` state at `EpicChatPanel.tsx:65` already disables input \u2014 extend it to show stop button\n- The `StreamingMarkdown` component at `renderer/components/markdown/StreamingMarkdown.tsx` uses block-memoization for efficient streaming render \u2014 reuse it for chat message rendering\n- `titlebar-no-drag` class must be applied to the stop button if it's in the top 50px (per `.flow/memory/electron-titlebar-clickability.md`)\n## Acceptance\n- [ ] `simulateAIResponse()` function deleted from EpicChatPanel.tsx\n- [ ] `/interview`, `/review`, and free-form messages route to real agent via `flowEpicChatSend`\n- [ ] `/plan` still routes to existing `FLOW_EPIC_PLAN` channel (unchanged)\n- [ ] Streaming text renders token-by-token via `updateLastMessage`\n- [ ] Stop button appears during processing and aborts the in-flight stream\n- [ ] Switching epics aborts any in-flight stream for the previous epic\n- [ ] Conversation history (last 20 messages) sent with each request\n- [ ] Error states (rate limit, network, auth) display as error message bubbles with retry option\n- [ ] Auto-scroll only when user is near bottom of chat\n- [ ] No stale closure issues with message persistence\n- [ ] TypeScript compiles with no errors\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-08T03:17:48.277288Z",
        "depends_on": [
          "fn-4-real-ai-epic-chat-with-project-aware.2"
        ],
        "epic": "fn-4-real-ai-epic-chat-with-project-aware",
        "id": "fn-4-real-ai-epic-chat-with-project-aware.3",
        "priority": null,
        "spec_path": ".flow/tasks/fn-4-real-ai-epic-chat-with-project-aware.3.md",
        "status": "todo",
        "title": "Smart Empty State + Starter Prompts",
        "updated_at": "2026-02-08T03:18:54.100106Z"
      },
      "id": "fn-4-real-ai-epic-chat-with-project-aware.3",
      "runtime": null,
      "spec": "# fn-4-real-ai-epic-chat-with-project-aware.3 Smart Empty State + Starter Prompts\n\n## Description\nReplace the minimal empty chat state with a rich, project-aware experience. Show dynamic starter prompts based on epic state that users can click to immediately start a conversation.\n\n**Size:** S\n**Files:**\n- `apps/electron/src/renderer/components/tasks/EpicChatPanel.tsx` (empty state section at L415-422)\n\n## Approach\n\n- Replace the static empty state at `EpicChatPanel.tsx:415-422` with dynamic starter prompts\n- Determine starter prompts based on epic state:\n  - **No tasks**: \"Break down this epic into tasks\" (\u2192 /plan), \"What questions should I answer first?\" (\u2192 /interview)\n  - **Has tasks, none done**: \"Review the task breakdown\" (\u2192 /review), \"What should I tackle first?\"\n  - **Has stuck tasks**: \"Help me get unstuck on [task title]\", \"What's blocking progress?\"\n  - **All tasks done**: \"What could we improve?\", \"Generate a retrospective\"\n- Include project name from epic context (already available via `epicsAtom`) in the greeting: \"What would you like to know about {epicTitle}?\"\n- Render starters as clickable pill buttons (follow pattern from `AISuggestionSidebar.tsx:60-161` for epic state detection)\n- On click: populate input draft and immediately call `handleSend()` (or use existing `handleInsertCommand` callback for slash commands)\n- Staggered fade-in animation using `motion.div` with `delay: index * 0.05` (standard pattern per GitHub scout findings)\n- Keep the empty state lightweight \u2014 no LLM calls at render time\n\n## Key context\n\n- Current empty state at `EpicChatPanel.tsx:415-422` is a simple centered message with icon\n- `AISuggestionSidebar.tsx:60-161` already has rule-based epic state detection logic \u2014 reuse the same conditions\n- `handleInsertCommand` callback exists for populating input with slash commands\n- `ListView.tsx:39-85` has a separate `EmptyTasksState` with \"Open Chat & Run /plan\" button \u2014 these should be consistent\n- Use `motion/react` for animations (already imported in the component)\n## Acceptance\n- [ ] Empty chat shows project-aware greeting with epic title\n- [ ] 2-4 dynamic starter prompts shown based on epic state (no tasks, has tasks, stuck, all done)\n- [ ] Clicking a starter prompt sends the message (populates input + triggers send)\n- [ ] Starter prompts use staggered fade-in animation\n- [ ] No LLM call at render time (pure rule-based)\n- [ ] Slash command starters (e.g., /plan, /review) route correctly through existing command parsing\n- [ ] Empty state disappears after first message is sent\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-08T03:17:48.399898Z",
        "depends_on": [
          "fn-4-real-ai-epic-chat-with-project-aware.1"
        ],
        "epic": "fn-4-real-ai-epic-chat-with-project-aware",
        "id": "fn-4-real-ai-epic-chat-with-project-aware.4",
        "priority": null,
        "spec_path": ".flow/tasks/fn-4-real-ai-epic-chat-with-project-aware.4.md",
        "status": "todo",
        "title": "Cross-Project Knowledge Context",
        "updated_at": "2026-02-08T03:19:12.667435Z"
      },
      "id": "fn-4-real-ai-epic-chat-with-project-aware.4",
      "runtime": null,
      "spec": "# fn-4-real-ai-epic-chat-with-project-aware.4 Cross-Project Knowledge Context\n\n## Description\nEnhance the epic chat agent's system prompt with cross-project knowledge. Read learnings and patterns from all registered flow projects and include them as context so the AI can suggest improvements based on what's worked in other projects.\n\n**Size:** M\n**Files:**\n- `apps/electron/src/main/lib/epic-chat-agent.ts` (enhance system prompt builder)\n- `apps/electron/src/main/ipc.ts` (add helper to read cross-project data)\n- `apps/electron/src/shared/types.ts` (add IPC channel if needed for cross-project read)\n\n## Approach\n\n- In `epic-chat-agent.ts`, add a `gatherCrossProjectContext()` function that:\n  1. Gets list of registered flow projects from `registeredFlowProjectsAtom` (or via IPC from the FlowBridge instances cached per workspace in `ipc.ts`)\n  2. For each project (excluding current): read `{projectRoot}/learnings.md` if it exists (via `fs.readFile`), read `.flow/memory/*.md` files if they exist\n  3. Aggregate learnings into a structured context block: project name, key learnings/decisions, patterns\n  4. Cap total cross-project context to ~2000 tokens (roughly 8000 chars) to avoid token bloat \u2014 prioritize projects with more learnings, truncate individual entries\n- Add this context to the system prompt under a \"Cross-Project Knowledge\" section:\n  ```\n  ## Patterns from other projects\n  These learnings come from other projects in this workspace. Suggest relevant improvements where applicable:\n  [aggregated learnings]\n  ```\n- Also include current project's `learnings.md` and `.flow/memory/` entries under \"Current Project Context\"\n- Use `RegisteredFlowProject` type at `shared/types.ts:827-831` which has `{ name, path, active }` \u2014 iterate over all registered projects\n- Handle missing files gracefully (many projects won't have learnings.md yet)\n- Cache cross-project context per session (don't re-read on every message) \u2014 use a simple `Map<workspaceRoot, { context, timestamp }>` with 5-minute TTL\n\n## Key context\n\n- `learnings.ts` at `packages/shared/src/agent/learnings.ts` extracts decisions/patterns from agent sessions \u2014 these get written to `{workspaceRoot}/learnings.md`\n- `registeredFlowProjectsAtom` stores all known projects \u2014 but this is a renderer atom. In the main process, registered projects are accessed via `flow-bridge.ts` instances cached in `ipc.ts`\n- `.flow/memory/` directory may contain topic-specific memory files (e.g., `electron-titlebar-clickability.md`)\n- The `FLOW_READ_PROJECT_CONTEXT` handler at `ipc.ts:2882-2919` already reads package.json name + README \u2014 extend or follow this pattern\n## Acceptance\n- [ ] System prompt includes learnings from current project's `learnings.md` and `.flow/memory/` files\n- [ ] System prompt includes learnings from other registered flow projects (excluding current)\n- [ ] Cross-project context capped at ~2000 tokens with prioritization\n- [ ] Missing files handled gracefully (no errors if learnings.md doesn't exist)\n- [ ] Context cached with 5-minute TTL (not re-read on every message)\n- [ ] AI responses reference cross-project patterns when relevant (verified manually)\n- [ ] TypeScript compiles with no errors\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    }
  ]
}
